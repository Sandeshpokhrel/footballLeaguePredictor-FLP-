{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10889179,"sourceType":"datasetVersion","datasetId":6404764},{"sourceId":11671066,"sourceType":"datasetVersion","datasetId":7324477},{"sourceId":11675990,"sourceType":"datasetVersion","datasetId":7328078}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport pandas as pd\nimport numpy as np\nimport os\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# *DATA PREPROCESSING AND FEATURE ENGINEERING* ","metadata":{}},{"cell_type":"code","source":"def preprocess_data(path):\n    \"\"\"\n    Preprocess the data by loading it from a CSV file, renaming columns, and converting data types.\n    \"\"\"\n    df = pd.read_csv(path)\n    df.sort_values(['date'], inplace=True)\n    df.reset_index(drop=True, inplace=True)\n    df['date'] = pd.to_datetime(df['date'])\n    df['result'] = pd.to_numeric(df['result'].replace({'w': 3, 'd': 1, 'l': 0}), errors='coerce')\n    df['result'] = df['result'].fillna(0).round().astype(int)\n    df.head(20)\n\n    # Safe division function\n    def safe_divide(a, b):\n        return np.divide(a, b, out=np.zeros_like(a), where=b!=0)\n\n    # Create team views with safe calculations\n    team_cols = ['date', 'team', 'team_form', 'xG', 'shots', 'shotOnTarget', 'deep', 'is_home', 'possession_proxy', 'final_pos']\n    \n    df_home = df.assign(\n        team=df['home_team'],\n        team_form=df['result'],\n        is_home=1,\n        final_pos= df['home_final_pos'],\n        possession_proxy=(\n            0.3 * safe_divide(df['xG_home'], df['xG_home']+df['xG_away']) +\n            0.3 * safe_divide(df['home_shots'], df['home_shots']+df['away_shots']) +\n            0.4 * safe_divide(df['home_deep'], df['home_deep']+df['away_deep'])\n        )\n    ).rename(columns={\n        'xG_home': 'xG',\n        'home_shots': 'shots',\n        'home_shotOnTarget': 'shotOnTarget',\n        'home_deep': 'deep'\n    })[team_cols]\n\n    df_away = df.assign(\n        team=df['away_team'],\n        team_form=df['result'].replace({3: 0, 0: 3}),\n        is_home=0,\n        final_pos= df['away_final_pos'],\n        possession_proxy=(\n            0.3 * safe_divide(df['xG_away'], df['xG_home']+df['xG_away']) +\n            0.3 * safe_divide(df['away_shots'], df['home_shots']+df['away_shots']) +\n            0.4 * safe_divide(df['away_deep'], df['home_deep']+df['away_deep'])\n        )\n    ).rename(columns={\n        'xG_away': 'xG',\n        'away_shots': 'shots',\n        'away_shotOnTarget': 'shotOnTarget',\n        'away_deep': 'deep'\n    })[team_cols]\n\n    # Combine and sort\n    df_team = pd.concat([df_home, df_away]).sort_values(['team','date']).reset_index(drop=True)\n\n    # Calculate rolling features\n    stats = ['xG', 'shots', 'shotOnTarget', 'deep', 'team_form', 'possession_proxy']\n    for stat in stats:\n        df_team[f'rolling_{stat}'] = (\n            df_team.groupby('team')[stat]\n            .transform(lambda x: x.rolling(window=5, min_periods=1).mean().shift(1))\n            # Optional: fill initial NaN per team\n            .fillna(0)\n        )\n    return df_team, df\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"team_df, match_df = preprocess_data('/kaggle/input/epl-with-labels-matchday/EPL.csv')\nteam_df.groupby('team', as_index=False).head(50)\nmatch_df.head(5)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def merge(match_df, team_df):\n    # print(team_df.columns.tolist())\n    prepared_df = match_df.merge(\n        team_df[team_df['is_home']==1][['date', 'team', *[f'rolling_{stat}' for stat in ['xG', 'shots', 'shotOnTarget', 'deep', 'team_form', 'possession_proxy']]]],\n        left_on=['date', 'home_team'],\n        right_on=['date', 'team'],\n        suffixes=('', '_home')\n    )\n    prepared_df = prepared_df.merge(\n        team_df[team_df['is_home']==0][['date', 'team', *[f'rolling_{stat}' for stat in ['xG', 'shots', 'shotOnTarget', 'deep', 'team_form', 'possession_proxy']]]],\n        left_on=['date', 'away_team'],\n        right_on=['date', 'team'],\n        suffixes=('', '_away')\n    )\n    return prepared_df\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"prepared_df = merge(match_df, team_df)\nteam_df.groupby('team').first()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"elo_df = pd.read_csv('/kaggle/input/club-football-match-data-2000-2025/EloRatings.csv')\nelo_df['date'] = pd.to_datetime(elo_df['date'])  # Convert first if needed\nelo_df[\n    (elo_df['country'] == \"ENG\") & \n    (elo_df['date'].dt.year == 2020) & (elo_df['club'] == \"Tottenham\")\n]\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from scipy.special import expit as sigmoid\n\n# Configuration\nk = 400  # Elo rating scale factor\nelo_baseline = 1500  # Baseline Elo rating\nstats_cols = ['xG', 'shots', 'shotOnTarget', 'deep', 'team_form', 'possession_proxy']\n# 1. Prepare the base statistics (using year-specific means)\ndef get_year_specific_base_stats(year):\n    year_mask = (team_df['date'].dt.year == year)\n    return {\n        'xG': team_df.loc[year_mask, 'rolling_xG'].mean(),\n        'shots': team_df.loc[year_mask, 'rolling_shots'].mean(),\n        'shotOnTarget': team_df.loc[year_mask, 'rolling_shotOnTarget'].mean(),\n        'deep': team_df.loc[year_mask, 'rolling_deep'].mean(),\n        'team_form': team_df.loc[year_mask, 'rolling_team_form'].mean(),\n        'possession_proxy': team_df.loc[year_mask, 'rolling_possession_proxy'].mean()\n    }\n\n# 2. Define the imputation function\ndef elo_impute(base_stat, elo_team, k=400, elo_baseline=1500):\n    return base_stat * (1 + sigmoid((elo_team - elo_baseline) / k))\n\n# 3. Process each row that needs imputation\nfor idx, row in team_df.iterrows():\n    if all(row[f'rolling_{col}'] == 0 for col in stats_cols):  # Check if any rolling stat is 0\n        year = row['date'].year\n        team = row['team']\n        \n        # Get year-specific base stats\n        base_stats = get_year_specific_base_stats(year)\n        \n       # Find team's Elo rating (most recent before current date)\n        filtered_elo = elo_df[\n            (elo_df['club'] == team) & \n            (elo_df['date'] <= row['date'])\n        ].sort_values('date', ascending=False)\n\n        # Use baseline if no past Elo found\n        team_elo = filtered_elo['elo'].values[0] if not filtered_elo.empty else elo_baseline\n        \n        # Impute each stat\n        for stat in stats_cols:\n            if row[f'rolling_{stat}'] == 0:\n                base_stat = base_stats[stat]\n                team_df.at[idx, f'rolling_{stat}'] = elo_impute(base_stat, team_elo, k, elo_baseline)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"team_df.head(50)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"prepared_df = merge(match_df, team_df)\nprepared_df.groupby('home_team').first()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#prepared_df = prepared_df.drop(['team_away', 'is_home'], axis=1)\nprepared_df.columns\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"prepared_df.sort_values(by=['team', 'date']).head(20).filter(['date', 'home_team', 'away_team', 'team'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"team_df.sort_values(by=['team', 'date']).head(20)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train1_df = team_df.merge(\n    prepared_df[['date','home_team', 'away_team', 'xG_away', *[f'away_{stat}' for stat in ['shots', 'shotOnTarget', 'deep']], *[f'rolling_{stat}_away' for stat in ['xG', 'shots', 'shotOnTarget', 'deep', 'team_form', 'possession_proxy']]]], \n    left_on= ['date', 'team'],\n    right_on = ['date', 'home_team'],\n    how = 'left'\n)\n#train_df.head(10).filter(['date', 'team', 'away_team', 'rolling_xG_away'])\ntrain1_df.columns\nrename_dict = {\n                'team_form': 'form',\n               'away_team': 'opponent',\n               'xG_home': 'xG',\n               'xG_away': 'opponent_xG',\n               **{f'away_{stat}': f'opponent_{stat}' for stat in ['shots', 'shotOnTarget', 'deep', 'team_form', 'possession_proxy']},\n               **{f'rolling_{stat}_away': f'opponent_rolling_{stat}' for stat in ['xG', 'shots', 'shotOnTarget', 'deep', 'team_form', 'possession_proxy']}}\ntrain1_df = train1_df.rename(columns =rename_dict)\ntrain1_df.drop(columns = ['home_team'])\ntrain1_df.columns","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train1_df[train1_df['team']== 'Crystal Palace'].head(5)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train2_df = team_df.merge(\n    prepared_df[['date','home_team', 'away_team', 'xG_home', *[f'home_{stat}' for stat in ['shots', 'shotOnTarget', 'deep']], *[f'rolling_{stat}' for stat in ['xG', 'shots', 'shotOnTarget', 'deep', 'team_form', 'possession_proxy']]]], \n    left_on= ['date', 'team'],\n    right_on = ['date', 'away_team'],\n    how = 'left',\n    suffixes = ['','_home']\n)\n#train_df.head(10).filter(['date', 'team', 'away_team', 'rolling_xG_away'])\ntrain2_df.columns\nrename_dict = {\n                'team_form': 'form',\n               'home_team': 'opponent',\n               'xG_home': 'opponent_xG',\n               **{f'home_{stat}': f'opponent_{stat}' for stat in ['shots', 'shotOnTarget', 'deep', 'team_form', 'possession_proxy']},\n               **{f'rolling_{stat}_home': f'opponent_rolling_{stat}' for stat in ['xG', 'shots', 'shotOnTarget', 'deep', 'team_form', 'possession_proxy']}}\ntrain2_df = train2_df.rename(columns =rename_dict)\ntrain2_df.head(5)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"final_df = train1_df.copy()\n\n# Fill in missing values in train1_df with values from train2_df\nfinal_df = final_df.fillna(train2_df)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"final_df.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"final_df.to_csv('csv_file.csv')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#arrangements according to the season instead of date.\nfinal_df['date'] = pd.to_datetime(final_df['date'])\nfinal_df['season'] = final_df['date'].apply(lambda x: x.year if x.month >= 8 else x.year-1)\n\nfinal_df = final_df.sort_values(['season', 'team', 'date'])\ngrouped = final_df.groupby(['team', 'season'])\n\nfinal_df.groupby('team').head(2)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"final_df = final_df.sort_values([\"season\", \"team\", \"date\"]).reset_index()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"final_df.filter([\n    'season',\n    'team',\n    'opponent',\n    'final_pos'\n]).groupby(\n    'season'\n).head(1)\nfinal_df.columns","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"final_df=final_df.drop(columns=['home_team'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"final_df.columns\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install scikit-learn\n# !pip install --upgrade tensorflow","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelEncoder\nscale_features = [\n        'form', 'xG', 'shots', 'shotOnTarget', 'deep',\n       'possession_proxy', 'rolling_xG', 'rolling_shots',\n       'rolling_shotOnTarget', 'rolling_deep', 'rolling_team_form',\n       'rolling_possession_proxy', 'opponent_xG', 'opponent_shots',\n       'opponent_shotOnTarget', 'opponent_deep', 'opponent_rolling_xG',\n       'opponent_rolling_shots', 'opponent_rolling_shotOnTarget',\n       'opponent_rolling_deep', 'opponent_rolling_team_form',\n       'opponent_rolling_possession_proxy'\n]\n#encoding certain categorical features\nfinal_df['final_pos'] = final_df['final_pos'] - 1\n\n# Training set: 2014-2020 (7 seasons)\ntrain_df = final_df[final_df[\"season\"].isin(range(2014, 2021))]  # 2014 to 2020 inclusive\n\n# Validation set: 2021 (1 season) - For hyperparameter tuning\nval_df = final_df[final_df[\"season\"] == 2021]\n\n# Test set: 2022 (most recent season) - Final evaluation\ntest_df = final_df[final_df[\"season\"] == 2022]\n\nscaler = StandardScaler()\n# 1. Fit and transform TRAIN data\nscaled_train = scaler.fit_transform(train_df[scale_features])\nfinal_train_df = pd.concat([\n    pd.DataFrame(scaled_train, columns=scale_features, index=train_df.index),\n    train_df[['is_home', 'final_pos', 'date', 'season', 'team', 'opponent']]\n], axis=1)\n\n# 2. Transform VALIDATION data\nscaled_val = scaler.transform(val_df[scale_features])\nfinal_val_df = pd.concat([\n    pd.DataFrame(scaled_val, columns=scale_features, index=val_df.index),\n    val_df[['is_home', 'final_pos', 'date', 'season', 'team', 'opponent']]\n], axis=1)\n\n# 3. Transform TEST data\nscaled_test = scaler.transform(test_df[scale_features])\nfinal_test_df = pd.concat([\n    pd.DataFrame(scaled_test, columns=scale_features, index=test_df.index),\n    test_df[['is_home', 'final_pos', 'date', 'season', 'team', 'opponent']]\n], axis=1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"final_train_df.head(5)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.sequence import pad_sequences\nfeature_cols = [\n        'form', 'xG', 'shots', 'shotOnTarget', 'deep',\n       'possession_proxy', 'is_home', 'rolling_xG', 'rolling_shots',\n       'rolling_shotOnTarget', 'rolling_deep', 'rolling_team_form',\n       'rolling_possession_proxy', 'opponent_xG', 'opponent_shots',\n       'opponent_shotOnTarget', 'opponent_deep', 'opponent_rolling_xG',\n       'opponent_rolling_shots', 'opponent_rolling_shotOnTarget',\n       'opponent_rolling_deep', 'opponent_rolling_team_form',\n       'opponent_rolling_possession_proxy'\n]\ndef create_sequences(data, min_length = 5):\n    sequences, targets = [], []\n    grouped = data.groupby(['team', 'season'])\n    for (team, season), group in grouped:\n        group = group.sort_values('date')\n        X = group[feature_cols].values # it includes array 2-d (34, 23)\n        y = group['final_pos'].iloc[0]\n\n        for n in range(min_length, len(group)+1):\n            sequences.append(X[:n])\n            targets.append(y)\n    padded_sequences = pad_sequences(sequences, maxlen=34, padding=\"post\", truncating='post',dtype='float32')\n    return np.array(padded_sequences), np.array(targets)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Generate train/test sequences\nX_train, y_train = create_sequences(final_train_df)\nX_val, y_val = create_sequences(final_val_df)\nX_test, y_test = create_sequences(final_test_df)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install matplotlib seaborn\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"***DATA VISUALIZATION FOR SEQUENCES / TEMPORAL PATTERN***\n","metadata":{}},{"cell_type":"code","source":"\nteam_data = final_train_df[(train_df['team']== \"Chelsea\") & (final_train_df['season']== 2014)].sort_values(by='date')\n\nplt.figure(figsize=(15, 8))  # Single figure with subplots\n\nfor i, feature in enumerate(['rolling_team_form', 'rolling_xG', 'rolling_deep'], 1):\n    plt.subplot(3, 1, i)  # 3 rows, 1 column, position i\n    sns.lineplot(data=team_data, x='date', y=feature, marker='o', color='blue')\n    plt.title(f\"Chelsea 2014: {feature}\")\n    plt.xticks(rotation=45)\n    plt.grid(True)\n\nplt.tight_layout()  # Prevent overlapping labels\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Define teams and features to compare\nteams = [\"Chelsea\", \"Manchester City\", \"Liverpool\"]  # Add more teams if needed\nfeatures = ['form', 'xG', 'deep']\nseason = 2014\n\n# Set up subplots: 1 row per team, 3 columns per feature\nfig, axes = plt.subplots(len(teams), len(features), figsize=(18, 10), sharex=True)\n\nfor i, team in enumerate(teams):\n    # Filter team data for the season\n    team_data = final_train_df[\n        (final_train_df['team'] == team) & \n        (final_train_df['season'] == season)\n    ].sort_values(by='date')\n    \n    for j, feature in enumerate(features):\n        ax = axes[i, j]\n        sns.lineplot(\n            data=team_data, \n            x='date', \n            y=feature, \n            ax=ax,\n            marker='o',\n            color=['#FF6B6B', '#4ECDC4', '#45B7D1'][j]  # Different colors per feature\n        )\n        ax.set_title(f\"{team} {season}: {feature}\")\n        ax.set_ylabel(feature)\n        ax.tick_params(axis='x', rotation=45)\n        ax.grid(True)\n\nplt.tight_layout()\nplt.show()\n\n# Check for infinite values (unchanged from your code)\nnumeric_cols = final_train_df.select_dtypes(include=[np.number]).columns\ninf_counts = (np.isinf(final_train_df[numeric_cols])).sum()\nprint(\"Infinite values in numeric columns:\")\nprint(inf_counts)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_feature_distributions(features=['xG', 'shots', 'is_home']):\n    \"\"\"Compare feature distributions across train/val/test sets\"\"\"\n    plt.figure(figsize=(15, 4))\n    for i, feature in enumerate(features):\n        plt.subplot(1, len(features), i+1)\n        sns.kdeplot(train_df[feature], label='Train')\n        sns.kdeplot(val_df[feature], label='Validation')\n        sns.kdeplot(test_df[feature], label='Test')\n        plt.title(feature)\n        plt.legend()\n    plt.tight_layout()\n    plt.show()\n\nplot_feature_distributions()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_sequence(sequence_idx, max_timesteps=10, n_features=5):\n    \"\"\"Plot first N timesteps of a sample sequence\"\"\"\n    plt.figure(figsize=(12, 6))\n    \n    # Get first sequence (padded)\n    sample_seq = X_train[sequence_idx][:max_timesteps, :n_features]\n    \n    # Create heatmap\n    sns.heatmap(sample_seq.T, annot=True, cmap='viridis', \n                yticklabels=feature_cols[:n_features])\n    plt.title(f\"Sequence {sequence_idx} (First {max_timesteps} Games)\")\n    plt.xlabel(\"Timesteps (Games)\")\n    plt.ylabel(\"Features\")\n    plt.show()\n\n# Example: First sequence, first 5 features\nplot_sequence(32, max_timesteps=5, n_features=5)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Training mean:\", scaler.mean_[:5])  # First 5 features' means\nprint(\"Training std:\", scaler.scale_[:5])  # First 5 features' std devs\n\n# Check test data didn't influence scaling\nassert np.allclose(scaler.mean_, train_df[scale_features].mean()), \"Data leakage!\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(10, 4))\nsns.countplot(x=y_train)\nplt.title(\"Distribution of Final Positions (Training Set)\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"y_train:\", y_train[:35])\nprint(\"\\nUnique targets:\", np.unique(y_train))# Check possible positions (e.g., 1-20)\ny_train.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense, Dropout, Masking\nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.losses import SparseCategoricalCrossentropy\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n\nm = X_train.shape[0]\ntime_steps = X_train.shape[1]\nn = X_train.shape[2]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Last timestep (should be 0):\", X_train[0][-1, :3])  # First 3 features","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = Sequential([\n    Masking(mask_value=0.0, input_shape=(time_steps, n)),\n    LSTM(64, return_sequences = False, kernel_regularizer=l2(0.01)), #uses l2(ridge) regularization slower and smoother than l1 \n    Dropout(0.3), #factor 0.3 mean 30 percent of the neurons are skipped while training to avoid of.\n    Dense(20, activation= 'softmax')\n])\n\nmodel.compile(\n    optimizer= Adam(learning_rate=0.001),\n    loss = SparseCategoricalCrossentropy(),\n    metrics = ['accuracy']\n)\n\nmodel.summary()\n\nos.makedirs('/kaggle/working/checkpoints', exist_ok=True)\n\n# using EarlyStopping and ModelChekpoint callbacks\n\nearly_stopping = EarlyStopping(monitor='val_accuracy', patience=20, mode='max', restore_best_weights=True)\n\ncheckpoint = ModelCheckpoint(\n    \n    filepath='/kaggle/working/checkpoints/model_epoch{epoch:02d}.keras',\n    monitor='val_accuracy',\n    save_best_only=False,\n    save_weights_only=False,\n    verbose=1,\n    \n)\n\n#Model training\nmodel.fit(\n    X_train,\n    y_train,\n    epochs=80,\n    batch_size=32,\n    validation_data=(X_val, y_val),\n    callbacks=[early_stopping, checkpoint]\n)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}